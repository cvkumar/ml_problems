1) Come up with two interesting MDPs. Explain why they are interesting. They don't need to be overly complicated or
directly grounded in a real situation, but it will be worthwhile if your MDPs are inspired by some process you are
interested in or are familiar with. It's ok to keep it somewhat simple. For the purposes of this assignment, though,
make sure one has a "small" number of states, and the other has a "large" number of states. I'm not going to go into detail about what large is,
but 200 is not large. Furthermore, because I like variety no more than one of the MDPs you choose should be a so-called grid world problem.

-Two MDP problems
-why interesting
-One should have a small number of states and the other should have large (at least more than 200) number of states
-At least one has to be a non-grid problem

2) Solve each MDP using value iteration as well as policy iteration. How many iterations does it take to converge?
Which one converges faster? Why? How did you choose to define convergence? Do they converge to the same answer?
How did the number of states affect things, if at all?

-Solve both problems with value iteration and policy iteration.
-Compare performance ~ speed and/or iterations
-Compare convergence: Compare algos answers? How did you define convergence?
-Did number small vs. large number of states affect results?

3) Now pick your favorite reinforcement learning algorithm and use it to solve the two MDPs.
How does it perform, especially in comparison to the cases above where you knew the model, rewards, and so on?
What exploration strategies did you choose? Did some work better than others?

-Find RL algo and solve the two MDPs
-How did it's results compare to value and policy iteration
-Explain your exploration strategies and compare their effectiveness
